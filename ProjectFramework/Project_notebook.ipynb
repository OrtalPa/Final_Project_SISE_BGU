{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project: Meta Aggregation #\n",
    "\n",
    "### by: Ortal & Raz & Michal\n",
    "### guidence: Hila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from GetProcessedData import get_question_dfs\n",
    "from AggregationMethods.ConfidenceMethods import highest_average_confidence\n",
    "from AggregationMethods.SurprisinglyPopular import surprisingly_pop_answer\n",
    "from AggregationMethods.MajorityRule import majority_answer\n",
    "from FeaturesExtraction.AnswerFeatures import AnswerF\n",
    "from FeaturesExtraction.AnswerFeaturesSubgroups import AnswerSubF\n",
    "from FeaturesExtraction.ConfidenceFeatures import ConfidenceF\n",
    "from FeaturesExtraction.ConfidenceFeaturesSubgroups import ConfidenceSubF\n",
    "from FeaturesExtraction.PredictionFeatures import PredictionsF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [0 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [0 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 0 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 0]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 0]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [0 1 0]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [0 1 0]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n",
      "0.5480769230769231\n",
      "training\n",
      "predicting\n",
      "0.7403846153846154\n",
      "Test set R^2:  0.7403846153846154\n",
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (15, 0)\t1.0\n",
      "  (16, 0)\t1.0\n",
      "  (17, 0)\t1.0\n",
      "  (19, 0)\t1.0\n",
      "  (21, 0)\t1.0\n",
      "  (22, 0)\t1.0\n",
      "  (25, 0)\t1.0\n",
      "  (26, 0)\t1.0\n",
      "  (27, 0)\t1.0\n",
      "  (28, 0)\t1.0\n",
      "  (29, 0)\t1.0\n",
      "  (31, 0)\t1.0\n",
      "  (32, 0)\t1.0\n",
      "  (34, 0)\t1.0\n",
      "  (35, 0)\t1.0\n",
      "  :\t:\n",
      "  (75, 2)\t1.0\n",
      "  (77, 2)\t1.0\n",
      "  (78, 2)\t1.0\n",
      "  (79, 2)\t1.0\n",
      "  (81, 2)\t1.0\n",
      "  (82, 2)\t1.0\n",
      "  (83, 2)\t1.0\n",
      "  (84, 2)\t1.0\n",
      "  (85, 2)\t1.0\n",
      "  (87, 2)\t1.0\n",
      "  (88, 2)\t1.0\n",
      "  (89, 2)\t1.0\n",
      "  (91, 2)\t1.0\n",
      "  (92, 2)\t1.0\n",
      "  (93, 2)\t1.0\n",
      "  (94, 2)\t1.0\n",
      "  (95, 2)\t1.0\n",
      "  (96, 2)\t1.0\n",
      "  (97, 2)\t1.0\n",
      "  (98, 2)\t1.0\n",
      "  (99, 2)\t1.0\n",
      "  (100, 2)\t1.0\n",
      "  (101, 2)\t1.0\n",
      "  (102, 2)\t1.0\n",
      "  (103, 2)\t1.0\n",
      "ClassifierChain - RandomForestClassifier\n",
      "Accuracy: 0.7403846153846154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85        68\n",
      "           1       0.85      0.98      0.91        87\n",
      "           2       0.78      0.96      0.86        76\n",
      "\n",
      "   micro avg       0.82      0.94      0.88       231\n",
      "   macro avg       0.82      0.94      0.87       231\n",
      "weighted avg       0.82      0.94      0.88       231\n",
      " samples avg       0.80      0.83      0.80       231\n",
      "\n",
      "MLPClassifier\n",
      "Accuracy: 0.5480769230769231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.68      0.71        68\n",
      "           1       0.83      0.99      0.91        87\n",
      "           2       0.73      0.91      0.81        76\n",
      "\n",
      "   micro avg       0.78      0.87      0.82       231\n",
      "   macro avg       0.77      0.86      0.81       231\n",
      "weighted avg       0.78      0.87      0.82       231\n",
      " samples avg       0.77      0.77      0.74       231\n",
      "\n",
      "RandomForestClassifier\n",
      "Accuracy: 0.6730769230769231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81        68\n",
      "           1       0.84      0.94      0.89        87\n",
      "           2       0.76      0.93      0.84        76\n",
      "\n",
      "   micro avg       0.80      0.91      0.85       231\n",
      "   macro avg       0.80      0.90      0.85       231\n",
      "weighted avg       0.80      0.91      0.85       231\n",
      " samples avg       0.76      0.79      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from ProjectFramework.GetProcessedData import get_question_dfs\n",
    "from ProjectFramework.Pipeline.MultiLabelPipeline import create_data_df\n",
    "\n",
    "#import ProjectFramework.Pipeline.MultiLabelPipeline.MultiLabelPipeline.create_data_df\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\school & work\\PycharmProjects\\Final_Project_SISE_BGU\\ProjectFramework\\Pipeline\\MultiLabelPipeline.py\", line 114, in create_data_df\n",
      "    correct_answer = str(df[df['Class'] == 1]['Answer'].iloc[0])\n",
      "  File \"c:\\python38\\lib\\site-packages\\pandas\\core\\indexing.py\", line 1768, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"c:\\python38\\lib\\site-packages\\pandas\\core\\indexing.py\", line 2138, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"c:\\python38\\lib\\site-packages\\pandas\\core\\indexing.py\", line 2063, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>AS_distribution_most_pop_ans</th>\n",
       "      <th>AS_entropy_distance_highest_lowest</th>\n",
       "      <th>AS_has_pop_ans_changed</th>\n",
       "      <th>AS_std_distance_highest_lowest</th>\n",
       "      <th>AS_var_distance_highest_lowest</th>\n",
       "      <th>A_above50%</th>\n",
       "      <th>A_distance1-2</th>\n",
       "      <th>A_distance1-last</th>\n",
       "      <th>A_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>P_highest_mean</th>\n",
       "      <th>P_highest_std</th>\n",
       "      <th>P_highest_var</th>\n",
       "      <th>P_low_con_high_p</th>\n",
       "      <th>P_lower_then_votes</th>\n",
       "      <th>P_lowest_mean</th>\n",
       "      <th>P_lowest_std</th>\n",
       "      <th>P_lowest_var</th>\n",
       "      <th>P_predicition</th>\n",
       "      <th>SP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>0.218756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.206648</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.091873</td>\n",
       "      <td>19.091873</td>\n",
       "      <td>2.115402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181957</td>\n",
       "      <td>0.231198</td>\n",
       "      <td>0.053453</td>\n",
       "      <td>4.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.043804</td>\n",
       "      <td>0.047476</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.126706</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.325765</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.327382</td>\n",
       "      <td>21.327382</td>\n",
       "      <td>1.262139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368878</td>\n",
       "      <td>0.240059</td>\n",
       "      <td>0.057628</td>\n",
       "      <td>4.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.128061</td>\n",
       "      <td>0.136663</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.134766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.934701</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.622996</td>\n",
       "      <td>34.622996</td>\n",
       "      <td>1.115028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462637</td>\n",
       "      <td>0.312786</td>\n",
       "      <td>0.097835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.058216</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.304301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.123525</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.242641</td>\n",
       "      <td>8.757359</td>\n",
       "      <td>1.933947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>0.234443</td>\n",
       "      <td>0.054963</td>\n",
       "      <td>4.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.079687</td>\n",
       "      <td>0.101195</td>\n",
       "      <td>0.010241</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.263845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.115926</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.088723</td>\n",
       "      <td>8.911277</td>\n",
       "      <td>1.302358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.271714</td>\n",
       "      <td>0.073829</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.151429</td>\n",
       "      <td>0.171125</td>\n",
       "      <td>0.029284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AS_distribution_most_pop_ans  AS_entropy_distance_highest_lowest  \\\n",
       "0  1.0                      0.004691                            0.218756   \n",
       "1  1.0                      0.001519                            0.126706   \n",
       "2  0.0                      0.001728                            0.134766   \n",
       "3  0.0                      0.004535                            0.304301   \n",
       "4  0.0                      0.008822                            0.263845   \n",
       "\n",
       "   AS_has_pop_ans_changed  AS_std_distance_highest_lowest  \\\n",
       "0                     0.0                        1.206648   \n",
       "1                     1.0                        1.325765   \n",
       "2                     1.0                        0.934701   \n",
       "3                     0.0                        1.123525   \n",
       "4                     0.0                        3.115926   \n",
       "\n",
       "   AS_var_distance_highest_lowest  A_above50%  A_distance1-2  \\\n",
       "0                        6.181818         1.0       7.091873   \n",
       "1                       11.500000         1.0       4.327382   \n",
       "2                       11.666667         1.0      11.622996   \n",
       "3                        4.000000         1.0      -2.242641   \n",
       "4                       16.500000         1.0      -4.088723   \n",
       "\n",
       "   A_distance1-last  A_entropy  ...  P_highest_mean  P_highest_std  \\\n",
       "0         19.091873   2.115402  ...        0.181957       0.231198   \n",
       "1         21.327382   1.262139  ...        0.368878       0.240059   \n",
       "2         34.622996   1.115028  ...        0.462637       0.312786   \n",
       "3          8.757359   1.933947  ...        0.199219       0.234443   \n",
       "4          8.911277   1.302358  ...        0.310000       0.271714   \n",
       "\n",
       "   P_highest_var  P_low_con_high_p  P_lower_then_votes  P_lowest_mean  \\\n",
       "0       0.053453               4.0                67.0       0.043804   \n",
       "1       0.057628               4.0                98.0       0.128061   \n",
       "2       0.097835               0.0                82.0       0.036703   \n",
       "3       0.054963               4.0                57.0       0.079687   \n",
       "4       0.073829               3.0                70.0       0.151429   \n",
       "\n",
       "   P_lowest_std  P_lowest_var  P_predicition   SP  \n",
       "0      0.047476      0.002254            2.0  1.0  \n",
       "1      0.136663      0.018677            0.0  0.0  \n",
       "2      0.058216      0.003389            4.0  0.0  \n",
       "3      0.101195      0.010241            3.0  1.0  \n",
       "4      0.171125      0.029284            0.0  0.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get compleate data frame with features:\n",
    "\n",
    "DATA = create_data_df()\n",
    "\n",
    "DATA.head()\n",
    "#X = DATA.drop(\"labels\")\n",
    "#y = DATA[\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test using regular split\n",
    "\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test using k-fold cross validation\n",
    "\n",
    "kf = KFold(n_splits=2, random_state=None, shuffle=False) # Define the split - into 2 folds \n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # print(“TRAIN:”, train_index, “TEST:”, test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    ## HERE - run moddels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test using leave one out validation\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # print(X_train, X_test, y_train, y_test)\n",
    "    ## HERE - run moddels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "# Perform k-fold cross validation\n",
    "k = 10\n",
    "scores = cross_val_score(model, x, y, cv=k)\n",
    "print “Cross-validated scores:”, scores\n",
    "    \n",
    "# Make cross validated predictions\n",
    "predictions = cross_val_predict(model, df, y, cv=k)\n",
    "plt.scatter(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## present accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## present accuracy\n",
    "accuracy_list = []\n",
    "\n",
    "## add all models accuracy to list\n",
    "accuracy_list.append(accuracy_score(nb_predict,y_test))\n",
    "accuracy_list.append(accuracy_score(knn_predict,y_test))\n",
    "accuracy_list.append(accuracy_score(rfc_predict,y_test))\n",
    "accuracy_list.append(accuracy_score(logreg_predict,y_test))\n",
    "accuracy_list.append(accuracy_score(svm_predict,y_test))\n",
    "accuracy_list.append(accuracy_score(adc_predict,y_test))\n",
    "\n",
    "accuracy_dataframe = pd.DataFrame(columns=[\"Model\",\"Accuracy\"])\n",
    "model_list = ['Naive_Bayes','KNN','Random_Forest','Logistic_Reg','SVM','Ada_Boost']\n",
    "\n",
    "for i in range(len(accuracy_list)):\n",
    "    accuracy_dataframe.loc[i] = model_list[i], accuracy_list[i]\n",
    "\n",
    "accuracy_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shap feature evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JS vis\n",
    "shap.initjs() \n",
    "\n",
    "## the best model:\n",
    "model = rt_model\n",
    "\n",
    "# create a tree explainer for Random Forest\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "#sample 100 observations from the test data\n",
    "observations = X_test.sample(100, random_state=42)\n",
    "\n",
    "#create an explainer\n",
    "shap_values = explainer.shap_values(observations)\n",
    "\n",
    "#generate a summary plot\n",
    "shap.summary_plot(shap_values[1], features=observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix visualization\n",
    "\n",
    "def plot_confusion_matrix(test_Y, predict_y):\n",
    "    C = confusion_matrix(test_Y, predict_y)\n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    B =(C/C.sum(axis=0))\n",
    "    plt.figure(figsize=(20,4))\n",
    "    labels = [1,2]\n",
    "    cmap=sns.light_palette(\"#45c6c4\")\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Precision matrix\")\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Recall matrix\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test, rfc_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
